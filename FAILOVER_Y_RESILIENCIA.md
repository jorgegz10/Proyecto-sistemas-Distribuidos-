# ğŸ”„ Sistema de Failover y Resiliencia

## âœ… Implementaciones de Resiliencia

### 1ï¸âƒ£ **Circuit Breaker** 
**UbicaciÃ³n:** `common/resilience/circuitBreaker.py`

**Funcionamiento:**
- **CERRADO** â†’ Funcionamiento normal
- **ABIERTO** â†’ DespuÃ©s de 3 fallos consecutivos (no envÃ­a peticiones)
- **MEDIO_ABIERTO** â†’ Tras 10 segundos, intenta reconectar

```python
# Estados:
- CERRADO: Servicio funcionando
- ABIERTO: Servicio caÃ­do (protege contra sobrecarga)
- MEDIO_ABIERTO: Probando si el servicio volviÃ³
```

---

### 2ï¸âƒ£ **Failover AutomÃ¡tico del Gestor de Almacenamiento**
**UbicaciÃ³n:** `gestor_carga/gestor.py`

**CaracterÃ­sticas:**
âœ… MÃºltiples endpoints de almacenamiento
âœ… RotaciÃ³n automÃ¡tica cuando uno falla
âœ… Circuit breaker por cada endpoint
âœ… Reintentos con timeout de 3 segundos
âœ… Detecta servicios caÃ­dos y usa el siguiente

**CÃ³mo funciona:**
1. Intenta conectar al endpoint actual
2. Si falla o timeout â†’ marca fallo en circuit breaker
3. Rota al siguiente endpoint
4. Repite hasta encontrar uno disponible
5. Si todos fallan â†’ devuelve error al cliente

---

### 3ï¸âƒ£ **Redundancia de Actores (PUB/SUB)**

**Para PrÃ©stamo, DevoluciÃ³n:**
- MÃºltiples actores se suscriben al mismo tÃ³pico
- ZMQ distribuye mensajes automÃ¡ticamente (round-robin)
- Si un actor cae, los otros siguen procesando

**Ejemplo:**
```
Gestor â†’ Publica "prestamo" 
         â†“
    Actor1 (PC2) âœ… procesa
    Actor2 (PC3) âŒ caÃ­do
    
Siguiente mensaje â†’ Actor1 procesa (Ãºnico disponible)
```

---

## ğŸ”§ ConfiguraciÃ³n de Failover

### **Configurar mÃºltiples gestores de almacenamiento:**

En `docker-compose.pc1.yml`, agrega variable de entorno:

```yaml
gestor_carga:
  environment:
    - GESTOR_ALMACENAMIENTO_ENDPOINTS=gestor_almacenamiento:5570,backup_almacen:5570
```

Esto permite tener un servidor de respaldo.

---

## ğŸ§ª Probar el Failover

### **Prueba 1: CaÃ­da del Gestor de Almacenamiento**

```powershell
# 1. Iniciar todo normalmente
docker-compose -f docker-compose.pc1.yml up -d

# 2. Ejecutar test
python test_sistema.py

# 3. Detener almacenamiento
docker stop gestor_almacenamiento

# 4. Ejecutar test nuevamente
python test_sistema.py
# VerÃ¡s mensajes de failover intentando reconectar

# 5. Reiniciar almacenamiento
docker start gestor_almacenamiento

# 6. El sistema se recupera automÃ¡ticamente
python test_sistema.py
```

### **Prueba 2: CaÃ­da de un Actor**

```powershell
# 1. En PC2, detener actor_prestamo
docker stop actor_prestamo_pc2

# 2. Desde PC1, ejecutar prÃ©stamo
python test_sistema.py

# Resultado: 
# - El mensaje se publica por PUB/SUB
# - Como no hay actor escuchando, no se procesa
# - Pero el gestor NO se bloquea (fire-and-forget)
```

---

## ğŸ“Š Logs de Failover

Cuando ocurre un fallo, verÃ¡s:

```
[Gestor] ğŸ”„ Conectando a tcp://gestor_almacenamiento:5570 (intento 1/4)
[Gestor] â±ï¸  Timeout en tcp://gestor_almacenamiento:5570
[Gestor] âš ï¸  Circuit breaker ABIERTO para tcp://gestor_almacenamiento:5570
[Gestor] ğŸ”„ Conectando a tcp://backup_almacen:5570 (intento 2/4)
[Gestor] âœ… ConexiÃ³n exitosa a tcp://backup_almacen:5570
```

---

## ğŸ¯ Resumen de Protecciones

| Componente | ProtecciÃ³n | RecuperaciÃ³n |
|------------|-----------|--------------|
| **Gestor Almacenamiento** | Circuit Breaker + Failover | AutomÃ¡tica (rota a backup) |
| **Actores (PUB/SUB)** | Redundancia | Manual (reiniciar contenedor) |
| **Gestor Carga** | N/A (punto Ãºnico crÃ­tico) | Requiere reinicio |
| **PostgreSQL** | VolÃºmenes persistentes | Datos no se pierden |

---

## ğŸš€ Mejoras Futuras (Opcional)

Si quieres mÃ¡s resiliencia:

1. **MÃºltiples Gestores de Carga** con load balancer
2. **PostgreSQL con rÃ©plicas** (primary + standby)
3. **Health checks activos** (ping periÃ³dico a servicios)
4. **MÃ©tricas y monitoring** (Prometheus + Grafana)

---

## âœ… Estado Actual

**YA IMPLEMENTADO:**
- âœ… Circuit Breaker funcional
- âœ… Failover del gestor de almacenamiento
- âœ… Timeouts en conexiones
- âœ… Reintentos automÃ¡ticos
- âœ… RotaciÃ³n de endpoints
- âœ… Redundancia de actores vÃ­a PUB/SUB

**El sistema estÃ¡ listo para manejar fallos de servicios.**
